import pandas as pd
from gensim import corpora
from gensim.models import LdaModel
from matplotlib.font_manager import FontProperties
import re
import ast # å¼•å…¥ Abstract Syntax Tree æ¨¡çµ„
from tqdm import tqdm

input_file = "processed_HYPE_data_ver2.csv"
df = pd.read_csv(input_file)
# ====================================================================
# âš ï¸ 1. è³‡æ–™è®€å–èˆ‡æº–å‚™ (è«‹æ ¹æ“šä½ çš„å¯¦éš›ç¨‹å¼ç¢¼ä¿®æ”¹é€™éƒ¨åˆ†)
# ====================================================================

# å‡è¨­ä½ çš„ DataFrame å·²ç¶“è¼‰å…¥ï¼Œä¸¦ä¸”å·²ç¶“å®Œæˆäº†æ‰€æœ‰çš„é è™•ç†å’Œåˆä½µæ­¥é©Ÿ
# ä¾‹å¦‚ï¼š df = pd.read_csv('your_data.csv')
# å‡è¨­ df['final_tokens'] æ¬„ä½æ˜¯ List of Strings é¡å‹

# ç¢ºä¿ 'final_tokens' æ¬„ä½ä¸­çš„æ¯å€‹å­—ä¸²éƒ½è¢«å®‰å…¨åœ°è©•ä¼°ç‚º Python åˆ—è¡¨
def convert_str_to_list(list_str):
    try:
        # ast.literal_eval æ¯” eval() æ›´å®‰å…¨ï¼Œå°ˆé–€ç”¨æ–¼è©•ä¼°å­—ä¸²ä¸­çš„åŸºæœ¬æ•¸æ“šçµæ§‹
        return ast.literal_eval(list_str)
    except (ValueError, TypeError):
        # å¦‚æœé‡åˆ° NaN æˆ–ç„¡æ³•è©•ä¼°çš„å­—ä¸²ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []

# æ‡‰ç”¨è½‰æ›ï¼Œé€™å°‡æ˜¯ä½ çš„æ–°æœ€çµ‚è©å½™æ¬„ä½
df['final_tokens_restored'] = df['final_tokens'].apply(convert_str_to_list)

# --------------------------------------------------------------------
# æ¥ä¸‹ä¾†çš„ LDA æµç¨‹ï¼Œè«‹ä½¿ç”¨é€™å€‹æ–°çš„é‚„åŸæ¬„ä½
# --------------------------------------------------------------------
# 1. å»ºç«‹æ–‡æª”é›†åˆ (List of Lists)
documents = df['final_tokens_restored'].tolist()

# 2. ç§»é™¤ç©ºæ–‡æª”ï¼ˆå®‰å…¨æ“ä½œï¼‰
documents = [doc for doc in documents if doc]
# ====================================================================
# 2. æ•¸æ“šé©—è­‰èˆ‡å®‰å…¨æª¢æŸ¥ (é¿å… ValueError: cannot compute LDA over an empty collection)
# ====================================================================
total_docs = len(documents)
empty_docs_count = sum(1 for doc in documents if not doc)
total_tokens = sum(len(doc) for doc in documents)

print("\n--- æ•¸æ“šæµå¤±æœ€çµ‚æª¢æŸ¥ ---")
print(f"æ–‡æª”ç¸½æ•¸ (æ­Œæ›²æ•¸): {total_docs}")
print(f"ç©ºåˆ—è¡¨æ–‡æª”æ•¸: {empty_docs_count}")
print(f"æ‰€æœ‰æ–‡æª”ä¸­è©å½™çš„ç¸½è¨ˆæ•¸: {total_tokens}")

if total_tokens == 0:
    print("ğŸš¨ è‡´å‘½éŒ¯èª¤ï¼šæ‰€æœ‰æ–‡æª”è©å½™ç¸½è¨ˆæ•¸ç‚º 0ã€‚è«‹æª¢æŸ¥ DataFrame åŸå§‹æ¬„ä½ã€‚")
    exit()  # åœæ­¢åŸ·è¡Œ

# --------------------------------------------------------------------
# ç§»é™¤ç©ºæ–‡æª”ï¼ˆå¦‚æœç©ºæ–‡æª”æ•¸é‡ä¸å¤šï¼Œé€™æ¨£å¯ä»¥é¿å…å®ƒå€‘å¹²æ“¾å¾ŒçºŒè™•ç†ï¼‰
documents = [doc for doc in documents if doc]
# --------------------------------------------------------------------


# ====================================================================
# 3. å»ºç«‹è©å…¸ (Dictionary) å’Œèªæ–™åº« (Corpus)
# ====================================================================

print("\né–‹å§‹å»ºç«‹è©å…¸...")
# ä½¿ç”¨æ‰€æœ‰æ–‡æª”å»ºç«‹è©å…¸
dictionary = corpora.Dictionary(documents)

# è©å½™éæ¿¾ï¼šä½¿ç”¨æœ€å¯¬é¬†çš„æ¢ä»¶ä¾†é¿å…ä¸Ÿå¤±æ ¸å¿ƒè©
dictionary.filter_extremes(
    no_below=2,  # è©å½™è‡³å°‘åœ¨ 2 é¦–æ­Œä¸­å‡ºç¾é
    no_above=0.99,  # è©å½™åªæœ‰åœ¨è¶…é 99% çš„æ­Œä¸­å‡ºç¾æ‰ç§»é™¤
    keep_n=None
)

print(f"âœ… è©å½™è¡¨å¤§å° (éæ¿¾å¾Œ): {len(dictionary)}")

# å»ºç«‹ BoW èªæ–™åº« (å°‡è©å½™è½‰æ›ç‚º (ID, Count) æ ¼å¼)
corpus = [dictionary.doc2bow(doc) for doc in documents]
print(f"âœ… èªæ–™åº«æ–‡æª”æ•¸: {len(corpus)}")

# ====================================================================
# 4. è¨“ç·´ LDA æ¨¡å‹ (LdaModel)
# ====================================================================

# âš ï¸ é—œéµåƒæ•¸ï¼š num_topics (å»ºè­°å¾ 10 é–‹å§‹å˜—è©¦)
NUM_TOPICS = 10

print(f"\né–‹å§‹è¨“ç·´ {NUM_TOPICS} å€‹ä¸»é¡Œçš„ LDA æ¨¡å‹...")

lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=NUM_TOPICS,
    random_state=42,  # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
    chunksize=100,
    passes=20,  # å¢åŠ è¿­ä»£æ¬¡æ•¸ä»¥æé«˜æ¨¡å‹å“è³ª
    alpha='auto',
    per_word_topics=False  # é€™è£¡é€šå¸¸ä¸éœ€è¦ per_word_topics
)

print("âœ… LDA æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

# ====================================================================
# 5. çµæœè§£è®€ï¼šæª¢è¦–ä¸»é¡Œ (ä½¿ç”¨ CJK å­—å‹ç¢ºä¿éŸ“æ–‡é¡¯ç¤º)
# ====================================================================

# âš ï¸ è«‹ç¢ºä¿ä½ çš„ FONT_PATH æŒ‡å‘ä¸€å€‹æ”¯æŒéŸ“æ–‡çš„å­—å‹ï¼Œä¾‹å¦‚ Malgun Gothic
FONT_PATH = 'C:\\Windows\\Fonts\\malgun.ttf'
try:
    cjk_font = FontProperties(fname=FONT_PATH)
except:
    print("âš ï¸ è­¦å‘Šï¼šç„¡æ³•è¼‰å…¥éŸ“æ–‡å­—å‹ã€‚çµ‚ç«¯æ©Ÿè¼¸å‡ºå¯èƒ½æœƒå‡ºç¾äº‚ç¢¼ã€‚")

print("\n--- LDA ä¸»é¡Œæ¨¡å‹çµæœ (Top 10 è©å½™) ---")

for idx, topic in lda_model.print_topics(num_words=10):
    # æ¸…ç†è¼¸å‡ºæ ¼å¼ï¼šç§»é™¤æ•¸å­—æ¬Šé‡å’Œå°æ•¸é»ï¼Œåªä¿ç•™è©å½™
    # ç¯„ä¾‹è¼¸å‡º: 0.050*"word" + 0.040*"word2"
    cleaned_topic = re.sub(r'\d\.\d{3}\*"', '', topic).replace('"', '').replace(' + ', ' / ')

    # æ‰“å°çµæœ (å¦‚æœçµ‚ç«¯æ©Ÿæ”¯æŒï¼ŒéŸ“æ–‡æœƒæ­£å¸¸é¡¯ç¤º)
    print(f"ğŸŒŸ ä¸»é¡Œ #{idx + 1}ï¼š")
    print(f"   {cleaned_topic}\n")

# ====================================================================
# 6. ä¸‹ä¸€æ­¥ï¼šä¸»é¡Œé€£è²«æ€§ (å¯é¸)
# ====================================================================

# é€™æ˜¯ä¸‹ä¸€éšæ®µå„ªåŒ–ä¸»é¡Œæ•¸é‡çš„é—œéµ
# from gensim.models.coherencemodel import CoherenceModel
# coherence_model_lda = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')
# coherence_lda = coherence_model_lda.get_coherence()
# print(f"ä¸»é¡Œé€£è²«æ€§åˆ†æ•¸ (Coherence Score): {coherence_lda}")